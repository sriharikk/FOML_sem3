import pandas as pd
import numpy as np
from mlxtend.plotting import plot_decision_regions
df = pd.DataFrame()
df[&#39;X1&#39;]=[1,2,3,4,5,6,6,7,9,9]
df[&#39;X2&#39;]=[5,3,6,8,1,9,5,8,9,2]
df[&#39;label&#39;]=[1,1,0,1,0,1,0,1,0,0]
import seaborn as sns
sns.scatterplot(x=df[&#39;X1&#39;],y=df[&#39;X2&#39;],hue=df[&#39;label&#39;])

df[&#39;weights&#39;]=1/df.shape[0]

from sklearn.tree import DecisionTreeClassifier

dt1 = DecisionTreeClassifier(max_depth=1)
x = df.iloc[:,0:2].values
y = df.iloc[:,2].values
# Step 2 - Train 1st Model
dt1.fit(x,y)
from sklearn.tree import plot_tree

plot_decision_regions (x,yclf=dt1, legend=2)

dfl&#39;y pred&#39;] = dt1.predict(x)

def calculate_model_weight(error):
return 0.5*np.log((1-error)/(error))

# Step - 3 Calculate model weight
alpha1 = calculate_model_weight(0.3)

alpha1

0.42364893019360184

# Step -4 Update weights
def update_row_weights(row,alpha=0.423):
if row[&#39;label&#39;] == row[&#39;y_pred&#39;]:
return row[&#39;weights&#39;]* np.exp(-alpha)
else:
return row[&#39;weights&#39;]* np.exp(alpha)

df[&#39;updated_weights&#39;] = df.apply(update_row_weights,axis=1)

df[&#39;normalized_weights&#39;].sum()

df[&#39;cumsum_upper&#39;] = np.cumsum(df[&#39;normalized_weights&#39;])
df[&#39;cumsum_lower&#39;]=df[&#39;cumsum_upper&#39;] - df[&#39;normalized_weights&#39;]
df[[&#39;X1&#39;,&#39;X2&#39;,&#39;label&#39;,&#39;weights&#39;,&#39;y_pred&#39;,&#39;updated_weights&#39;,&#39;cumsum_lower&#39;,&#39;cumsum

def create_new_dataset(df):
indices= []
for i in range(df.shape[0]):
a = np.random.random()
for index,row in df.iterrows():
if row[&#39;cumsum_upper&#39;]&gt;a and a&gt;row[&#39;cumsum_lower&#39;]:
indices.append(index)
return indices

index_values = create_new_dataset(df)
index_values

second_df = df.iloc[index_values,[0,1,2,3]]
second_df

dt2 = DecisionTreeClassifier(max_depth=1)

x = second_df.iloc[:,0:2].values
y = second_df.iloc[:,2].values

dt2.fit(x,y)

plot_tree(dt2)

plot_decision_regions(x, y, clf=dt2, legend=2)

second_df[&#39;y_pred&#39;] = dt2.predict(x)
second_df
alpha2 = calculate_model_weight(0.1)

alpha2

# Step 4 - Update weights

def
update_row_weights(row,alpha=1.09):
if row[&#39;label&#39;] == row[&#39;y_pred&#39;]:
return row[&#39;weights&#39;] * np.exp(-alpha)
else:
return row[&#39;weights&#39;] * np.exp(alpha)

second_df[&#39;updated_weights&#39;] = second_df.apply(update_row_weights,axis=1)
second_df

second_df[&#39;nomalized_weights&#39;].sum()

second_df[&#39;nomalized_weights&#39;].sum()

second_df[&#39;cumsum_upper&#39;] = np.cumsum(second_df[&#39;nomalized_weights&#39;])
second_df[&#39;cumsum_lower&#39;] = second_df[&#39;cumsum_upper&#39;] - second_df[&#39;nomalized_weights&#39;]
second_df[[&#39;X1&#39;,&#39;X2&#39;,&#39;label&#39;,&#39;weights&#39;,&#39;y_pred&#39;,&#39;nomalized_weights&#39;,&#39;cumsum_lower&#39;,&#39;cumsum_
upper&#39;]]


alpha3 = calculate_model_weight(0.7)
alpha3

print(alpha1,alpha2,alpha3)

query = np.array([1,5]).reshape(1,2)
dt1.predict(query)

dt2.predict(query)

dt3.predict(query)

alpha1*1 + alpha2*(1) + alpha3*(1)

np.sign(1.09)

query = np.array([9,9]).reshape(1,2)
dt1.predict(query)

dt2.predict(query)

dt3.predict(query)

alpha1*(1) + alpha2*(-1) + alpha3*(-1)

np.sign(-0.25)
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

np.random.seed(42)
X = np.random.rand(100, 1) - 0.5
y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)

df = pd.DataFrame()

df[&#39;X&#39;] = X.reshape(100)

71

df[&#39;y&#39;] = y

df

plt.scatter(df[&#39;X&#39;],df[&#39;y&#39;])
plt.title(&#39;X vs y&#39;)

Text(0.5, 1.0, &#39;X vs y&#39;)

df[&#39;pred1&#39;] = df[&#39;y&#39;].mean()
df

72

df[&#39;res1&#39;] = df[&#39;y&#39;] - df[&#39;pred1&#39;]

df

plt.scatter(df[&#39;X&#39;],df[&#39;y&#39;])
plt.plot(df[&#39;X&#39;],df[&#39;pred1&#39;],color=&#39;red&#39;)

73
from sklearn.tree import DecisionTreeRegressor

tree1 = DecisionTreeRegressor(max_leaf_nodes=8)

tree1.fit(df[&#39;X&#39;].values.reshape(100,1),df[&#39;res1&#39;].values)

DecisionTreeRegressor(max_leaf_nodes=8)

from sklearn.tree import plot_tree
plot_tree(tree1)
plt.show()

74

75

df[&#39;pred2&#39;] = 0.265458 + tree1.predict(df[&#39;X&#39;].values.reshape(100,1))
df

df[&#39;res2&#39;] = df[&#39;y&#39;] - df[&#39;pred2&#39;]
df

76

def gradient_boost(X,y,number,lr,count=1,regs=[],foo=None):
if number == 0:
return
else:
# do gradient boosting
if count &gt; 1:
y = y - regs[-1].predict(X)
else:

77

foo = y
tree_reg = DecisionTreeRegressor(max_depth=5, random_state=42)
tree_reg.fit(X, y)

regs.append(tree_reg)

x1 = np.linspace(-0.5, 0.5, 500)
y_pred = sum(lr * regressor.predict(x1.reshape(-1, 1)) for regressor in regs)

print(number)
plt.figure()
plt.plot(x1, y_pred, linewidth=2)
plt.plot(X[:, 0], foo,&quot;r&quot;)
plt.show()

gradient_boost(X,y,number-1,lr,count+1,regs,foo=foo)

np.random.seed(42)
X = np.random.rand(100, 1) - 0.5
y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)
gradient_boost(X,y,5,lr=1)
